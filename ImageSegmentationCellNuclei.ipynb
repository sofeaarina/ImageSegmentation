{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageSegmentationCellNuclei.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMoJyjxD+JKv6l3rfIeaO9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofeaarina/ImageSegmentation/blob/main/ImageSegmentationCellNuclei.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX_P5PlSzSBf"
      },
      "outputs": [],
      "source": [
        "#Import packages\n",
        "import tensorflow as tf\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import os, cv2, glob, datetime\n",
        "from scipy import io\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.1 Prepare file directory\n",
        "train_file_dir = r\"\"\n",
        "test_file_dir = r\"\"\n"
      ],
      "metadata": {
        "id": "-AVi1LBMzdNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2 Load the images list\n",
        "def load_images(file_path):\n",
        "    images=[]\n",
        "    for image_file in os.listdir(file_path):\n",
        "        img = cv2.imread(os.path.join(file_path,image_file))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img,(128,128))\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "train_image_dir = os.path.join(train_file_dir,'inputs')\n",
        "train_images = load_images(train_image_dir)\n",
        "test_image_dir = os.path.join(test_file_dir,'inputs')\n",
        "test_images = load_images(test_image_dir)\n",
        "\n",
        "#1.3 Load the masks list\n",
        "def load_masks(file_path):\n",
        "    masks=[]\n",
        "    for mask_file in os.listdir(file_path):\n",
        "        mask = cv2.imread(os.path.join(file_path,mask_file),cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask,(128,128))\n",
        "        masks.append(mask)\n",
        "    return masks\n",
        "\n",
        "train_mask_dir = os.path.join(train_file_dir,'masks')\n",
        "train_masks = load_masks(train_mask_dir)\n",
        "test_mask_dir = os.path.join(test_file_dir,'masks')\n",
        "test_masks = load_masks(test_mask_dir)"
      ],
      "metadata": {
        "id": "imf6_sx3zexF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.4 Convert the lists into numpy array\n",
        "train_images_np = np.array(train_images)\n",
        "train_masks_np = np.array(train_masks)\n",
        "test_images_np = np.array(test_images)\n",
        "test_masks_np = np.array(test_masks)"
      ],
      "metadata": {
        "id": "yxLf07PZziEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.5. Check some examples\n",
        "#for images\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(1,4):\n",
        "    plt.subplot(1,3,i)\n",
        "    img_plot = train_images[i]\n",
        "    plt.imshow(img_plot)\n",
        "    plt.axis('off')\n",
        "plt.show()   \n",
        "\n",
        "#for masks\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(1,4):\n",
        "    plt.subplot(1,3,i)\n",
        "    mask_plot = train_masks[i]\n",
        "    plt.imshow(mask_plot, cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()  "
      ],
      "metadata": {
        "id": "fxzjtY3DzkZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Data preprocessing\n",
        "#2.1. Expand the mask dimension\n",
        "train_masks_np_exp = np.expand_dims(train_masks_np,axis=-1)\n",
        "test_masks_np_exp = np.expand_dims(test_masks_np,axis=-1)\n",
        "#Check the mask output\n",
        "print(np.unique(train_masks[0]))"
      ],
      "metadata": {
        "id": "7PawLRmgzmq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2. Change the mask value (Encode into numerical encoding, binary 1 and 0)\n",
        "converted_masks_train = np.ceil(train_masks_np_exp/255)\n",
        "converted_masks_test = np.ceil(test_masks_np_exp/255)\n",
        "converted_masks_train = 1 - converted_masks_train\n",
        "converted_masks_test = 1 - converted_masks_test"
      ],
      "metadata": {
        "id": "fzZsUGG7zoq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.3. Normalize the images\n",
        "converted_images_train = train_images_np / 255.0\n",
        "converted_images_test = test_images_np/255.0"
      ],
      "metadata": {
        "id": "VzkA6gICzqRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.4. Do train-validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "SEED = 12345\n",
        "x_train,x_val,y_train,y_val = train_test_split(converted_images_train,converted_masks_train,test_size=0.2,random_state=SEED)\n"
      ],
      "metadata": {
        "id": "vlUcGYjkzsHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.5. Convert the numpy array into tensor slice\n",
        "train_x = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "val_x = tf.data.Dataset.from_tensor_slices(x_val)\n",
        "train_y = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "val_y = tf.data.Dataset.from_tensor_slices(y_val)\n",
        "test_x = tf.data.Dataset.from_tensor_slices(converted_images_test)\n",
        "test_y = tf.data.Dataset.from_tensor_slices(converted_masks_test)\n"
      ],
      "metadata": {
        "id": "LsIzem9FzuCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.6. Zip the tensor slice into ZipDataset\n",
        "train = tf.data.Dataset.zip((train_x,train_y))\n",
        "val = tf.data.Dataset.zip((val_x,val_y))\n",
        "test = tf.data.Dataset.zip((test_x,test_y))\n"
      ],
      "metadata": {
        "id": "cHmgms7Gzvrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.7. Convert into PrefetchDataset\n",
        "BATCH_SIZE = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BUFFER_SIZE = 1000\n",
        "STEPS_PER_EPOCH = 800//BATCH_SIZE\n",
        "VALIDATION_STEPS = 200//BATCH_SIZE\n",
        "\n",
        "train = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "train = train.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val = val.batch(BATCH_SIZE).repeat()\n",
        "val = val.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test = test.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "YcxEvSCfzxgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Create the model\n",
        "#3.1 Use a pretrained as feature extractor\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[128,128,3],include_top=False)\n",
        "\n",
        "#8.2. Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "    ]\n",
        "\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "#Define the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False\n",
        "\n",
        "#Define the upsampling path\n",
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "    ]\n",
        "\n",
        "def unet_model(output_channels:int):\n",
        "    inputs = tf.keras.layers.Input(shape=[128,128,3])\n",
        "    #Applying functional API\n",
        "    # Downsampling through the model\n",
        "    skips = down_stack(inputs)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "    \n",
        "    #Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        concat = tf.keras.layers.Concatenate()\n",
        "        x = concat([x,skip])\n",
        "    \n",
        "    #This is the last layer of the model (output layer)\n",
        "    last = tf.keras.layers.Conv2DTranspose(\n",
        "        filters=output_channels, kernel_size=3, strides=2, padding='same') #64x64 --> 128x128\n",
        "    \n",
        "    x = last(x)\n",
        "    \n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "OUTPUT_CLASSES = 2\n",
        "\n",
        "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam',loss=loss,metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#Use in iPython console \n",
        "tf.keras.utils.plot_model(model, show_shapes=True)\n"
      ],
      "metadata": {
        "id": "jEgHS8Ubzzbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create functions to show predictions\n",
        "\n",
        "def create_mask(pred_mask):\n",
        "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "    pred_mask = pred_mask[...,tf.newaxis]\n",
        "    return pred_mask[0]\n",
        "\n",
        "#7. Visualize some pictures as example\n",
        "def display(display_list):\n",
        "    plt.figure(figsize=(15,15))\n",
        "    title = ['Input Image','True Mask','Predicted Mask']\n",
        "    \n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1,len(display_list),i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "        plt.axis('off')\n",
        "        \n",
        "    plt.show()\n",
        "    \n",
        "for images, masks in train.take(2):\n",
        "    sample_image,sample_mask = images[0], masks[0]\n",
        "    display([sample_image,sample_mask])\n",
        "\n",
        "def show_predictions(dataset=None, num=1):\n",
        "    if dataset:\n",
        "        for image, mask in dataset.take(num):\n",
        "            pred_mask = model.predict(image)\n",
        "            display([image[0],mask[0],create_mask(pred_mask)])\n",
        "            \n",
        "    else:\n",
        "        display([sample_image,sample_mask,create_mask(model.predict(sample_image[tf.newaxis,...]))])\n",
        "        \n",
        "show_predictions()\n"
      ],
      "metadata": {
        "id": "x1rvGMDfz4Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a callback to help to display results during training\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self,epoch,logs=None):\n",
        "        clear_output(wait=True)\n",
        "        show_predictions()\n",
        "        print('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
        "\n",
        "#Tensorboard and EarlyStopping callbacks\n",
        "base_log_path = r\"\"\n",
        "log_dir = os.path.join(base_log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '__Project_4')\n",
        "tb = tf.keras.callbacks.TensorBoard(log_dir,histogram_freq=1,profile_batch=0)\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,verbose=0)\n"
      ],
      "metadata": {
        "id": "2DyUajUSz88U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Start to do training\n",
        "EPOCH = 200\n",
        "\n",
        "history = model.fit(train,epochs=EPOCH,steps_per_epoch=STEPS_PER_EPOCH,batch_size=BATCH_SIZE,\n",
        "                    validation_steps=VALIDATION_STEPS,\n",
        "                    validation_data=val,\n",
        "                    callbacks=[DisplayCallback(),tb,es])\n",
        "\n",
        "#%%\n",
        "#Test evaluation\n",
        "test_loss, test_accuracy = model.evaluate(test)\n",
        "print(f\"Test loss = {test_loss}\")\n",
        "print(f\"Test accuracy = {test_accuracy}\")\n",
        "\n",
        "#%%\n",
        "#Deploy model\n",
        "show_predictions(test,3)\n",
        "\n",
        "#%%\n",
        "from numba import cuda \n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "WU1qFbil0bS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}